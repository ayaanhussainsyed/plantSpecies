{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GATHER DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayaanhussain/Library/Python/3.11/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-*.parquet', 'validation': 'data/validation-00000-of-00001.parquet'}\n",
    "df = dd.read_parquet(\"hf://datasets/dpdl-benchmark/oxford_flowers102/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5020199192705477385\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 9132100708516715886\n",
      "physical_device_desc: \"device: 0, name: METAL, pci bus id: <undefined>\"\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 23:28:24.260577: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Pro\n",
      "2025-05-03 23:28:24.260600: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB\n",
      "2025-05-03 23:28:24.260610: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.00 GB\n",
      "2025-05-03 23:28:24.260659: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-05-03 23:28:24.261038: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "_EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "_EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "RandomStandardNormal: (RandomStandardNormal): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "AddV2: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "a: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "b: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "product_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "MatMul time: 0.007147073745727539\n",
      "Result shape: (1000, 1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 23:28:24.281971: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-05-03 23:28:24.281987: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-05-03 23:28:24.296307: I tensorflow/core/common_runtime/placer.cc:125] input: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-05-03 23:28:24.296352: I tensorflow/core/common_runtime/placer.cc:125] _EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.296355: I tensorflow/core/common_runtime/placer.cc:125] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.303782: I tensorflow/core/common_runtime/placer.cc:125] input: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.303790: I tensorflow/core/common_runtime/placer.cc:125] _EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.303792: I tensorflow/core/common_runtime/placer.cc:125] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.316921: I tensorflow/core/common_runtime/placer.cc:125] shape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-05-03 23:28:24.316930: I tensorflow/core/common_runtime/placer.cc:125] RandomStandardNormal: (RandomStandardNormal): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.316934: I tensorflow/core/common_runtime/placer.cc:125] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.402665: I tensorflow/core/common_runtime/placer.cc:125] x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.402674: I tensorflow/core/common_runtime/placer.cc:125] y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.402678: I tensorflow/core/common_runtime/placer.cc:125] Mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.402681: I tensorflow/core/common_runtime/placer.cc:125] z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.410580: I tensorflow/core/common_runtime/placer.cc:125] x: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.410588: I tensorflow/core/common_runtime/placer.cc:125] y: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.410591: I tensorflow/core/common_runtime/placer.cc:125] AddV2: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.410593: I tensorflow/core/common_runtime/placer.cc:125] z_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.416849: I tensorflow/core/common_runtime/placer.cc:125] a: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.416858: I tensorflow/core/common_runtime/placer.cc:125] b: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.416864: I tensorflow/core/common_runtime/placer.cc:125] MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
      "2025-05-03 23:28:24.416866: I tensorflow/core/common_runtime/placer.cc:125] product_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "a = tf.random.normal([1000, 1000])\n",
    "b = tf.random.normal([1000, 1000])\n",
    "\n",
    "start = time.time()\n",
    "c = tf.matmul(a, b)\n",
    "end = time.time()\n",
    "\n",
    "print(\"MatMul time:\", end - start)\n",
    "print(\"Result shape:\", c.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONVERTING DATA TO READABLE AND EDITABLE FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               image  label\n",
      "0  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...     72\n",
      "1  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...     84\n",
      "2  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...     70\n",
      "3  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...     51\n",
      "4  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...     48\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_pd = df.compute()\n",
    "print(df_pd.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1020 entries, 0 to 1019\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   image   1020 non-null   object\n",
      " 1   label   1020 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 16.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_pd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def decode_image(img_bytes):\n",
    "    return Image.open(io.BytesIO(img_bytes))\n",
    "\n",
    "# Example usage\n",
    "sample_image = decode_image(df_pd.iloc[6][\"image\"][\"bytes\"])\n",
    "sample_image.show()  # Just to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of classes: 102\n",
      "\n",
      "Number of images per class:\n",
      " label\n",
      "0      10\n",
      "1      10\n",
      "2      10\n",
      "3      10\n",
      "4      10\n",
      "       ..\n",
      "97     10\n",
      "98     10\n",
      "99     10\n",
      "100    10\n",
      "101    10\n",
      "Name: count, Length: 102, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Count images per class\n",
    "class_counts = df_pd['label'].value_counts().sort_index()\n",
    "\n",
    "# Total number of classes\n",
    "num_classes = class_counts.shape[0]\n",
    "\n",
    "print(f\"Total number of classes: {num_classes}\")\n",
    "print(\"\\nNumber of images per class:\\n\", class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels in the Dataset:\n",
      "Label: 72\n",
      "Label: 84\n",
      "Label: 70\n",
      "Label: 51\n",
      "Label: 48\n",
      "Label: 83\n",
      "Label: 42\n",
      "Label: 58\n",
      "Label: 40\n",
      "Label: 35\n",
      "Label: 60\n",
      "Label: 59\n",
      "Label: 95\n",
      "Label: 87\n",
      "Label: 23\n",
      "Label: 91\n",
      "Label: 75\n",
      "Label: 79\n",
      "Label: 24\n",
      "Label: 20\n",
      "Label: 64\n",
      "Label: 89\n",
      "Label: 100\n",
      "Label: 62\n",
      "Label: 16\n",
      "Label: 2\n",
      "Label: 41\n",
      "Label: 26\n",
      "Label: 45\n",
      "Label: 67\n",
      "Label: 1\n",
      "Label: 61\n",
      "Label: 54\n",
      "Label: 39\n",
      "Label: 7\n",
      "Label: 12\n",
      "Label: 29\n",
      "Label: 11\n",
      "Label: 43\n",
      "Label: 98\n",
      "Label: 63\n",
      "Label: 15\n",
      "Label: 55\n",
      "Label: 38\n",
      "Label: 36\n",
      "Label: 78\n",
      "Label: 3\n",
      "Label: 30\n",
      "Label: 57\n",
      "Label: 73\n",
      "Label: 25\n",
      "Label: 5\n",
      "Label: 53\n",
      "Label: 90\n",
      "Label: 0\n",
      "Label: 92\n",
      "Label: 9\n",
      "Label: 68\n",
      "Label: 8\n",
      "Label: 28\n",
      "Label: 50\n",
      "Label: 22\n",
      "Label: 96\n",
      "Label: 31\n",
      "Label: 47\n",
      "Label: 69\n",
      "Label: 34\n",
      "Label: 52\n",
      "Label: 21\n",
      "Label: 81\n",
      "Label: 49\n",
      "Label: 46\n",
      "Label: 65\n",
      "Label: 94\n",
      "Label: 32\n",
      "Label: 56\n",
      "Label: 77\n",
      "Label: 6\n",
      "Label: 86\n",
      "Label: 88\n",
      "Label: 33\n",
      "Label: 71\n",
      "Label: 27\n",
      "Label: 93\n",
      "Label: 99\n",
      "Label: 17\n",
      "Label: 80\n",
      "Label: 18\n",
      "Label: 66\n",
      "Label: 14\n",
      "Label: 101\n",
      "Label: 44\n",
      "Label: 74\n",
      "Label: 4\n",
      "Label: 85\n",
      "Label: 82\n",
      "Label: 10\n",
      "Label: 13\n",
      "Label: 37\n",
      "Label: 76\n",
      "Label: 19\n",
      "Label: 97\n"
     ]
    }
   ],
   "source": [
    "unique_labels = df_pd['label'].unique()\n",
    "\n",
    "print(\"Unique Labels in the Dataset:\")\n",
    "for label in unique_labels:\n",
    "    print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   image_size\n",
      "0  (667, 500)\n",
      "1  (666, 500)\n",
      "2  (500, 670)\n",
      "3  (505, 500)\n",
      "4  (672, 500)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Function to get the image size (width, height)\n",
    "def get_image_size(image_bytes):\n",
    "    image = Image.open(io.BytesIO(image_bytes))  # Convert byte data to image\n",
    "    return image.size  # Returns (width, height)\n",
    "\n",
    "# Check image sizes for the first few images in the dataframe\n",
    "df_pd['image_size'] = df_pd['image'].apply(lambda x: get_image_size(x['bytes']))\n",
    "\n",
    "# Display the image sizes\n",
    "print(df_pd[['image_size']].head())  # Display first 5 image sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA CLEANING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayaanhussain/Library/Python/3.11/lib/python/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "# Load the ViT model and feature extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=102)  # 102 classes in this dataset\n",
    "\n",
    "# Class label to name mapping (as provided)\n",
    "label_to_class = {\n",
    "    '0': 'pink primrose', '1': 'hard-leaved pocket orchid', '2': 'canterbury bells', '3': 'sweet pea', '4': 'english marigold', \n",
    "    '5': 'tiger lily', '6': 'moon orchid', '7': 'bird of paradise', '8': 'monkshood', '9': 'globe thistle',\n",
    "    '10': 'snapdragon', '11': 'colt\\'s foot', '12': 'king protea', '13': 'spear thistle', '14': 'yellow iris',\n",
    "    '15': 'globe-flower', '16': 'purple coneflower', '17': 'peruvian lily', '18': 'balloon flower', '19': 'giant white arum lily',\n",
    "    '20': 'fire lily', '21': 'pincushion flower', '22': 'fritillary', '23': 'red ginger', '24': 'grape hyacinth',\n",
    "    '25': 'corn poppy', '26': 'prince of wales feathers', '27': 'stemless gentian', '28': 'artichoke', '29': 'sweet william',\n",
    "    '30': 'carnation', '31': 'garden phlox', '32': 'love in the mist', '33': 'mexican aster', '34': 'alpine sea holly',\n",
    "    '35': 'ruby-lipped cattleya', '36': 'cape flower', '37': 'great masterwort', '38': 'siam tulip', '39': 'lenten rose',\n",
    "    '40': 'barbeton daisy', '41': 'daffodil', '42': 'sword lily', '43': 'poinsettia', '44': 'bolero deep blue',\n",
    "    '45': 'wallflower', '46': 'marigold', '47': 'buttercup', '48': 'oxeye daisy', '49': 'common dandelion',\n",
    "    '50': 'petunia', '51': 'wild pansy', '52': 'primula', '53': 'sunflower', '54': 'pelargonium', '55': 'bishop of llandaff',\n",
    "    '56': 'gaura', '57': 'geranium', '58': 'orange dahlia', '59': 'pink-yellow dahlia', '60': 'cautleya spicata',\n",
    "    '61': 'japanese anemone', '62': 'black-eyed susan', '63': 'silverbush', '64': 'californian poppy', '65': 'osteospermum',\n",
    "    '66': 'spring crocus', '67': 'bearded iris', '68': 'windflower', '69': 'tree poppy', '70': 'gazania', '71': 'azalea',\n",
    "    '72': 'water lily', '73': 'rose', '74': 'thorn apple', '75': 'morning glory', '76': 'passion flower', '77': 'lotus',\n",
    "    '78': 'toad lily', '79': 'anthurium', '80': 'frangipani', '81': 'clematis', '82': 'hibiscus', '83': 'columbine',\n",
    "    '84': 'desert-rose', '85': 'tree mallow', '86': 'magnolia', '87': 'cyclamen', '88': 'watercress', '89': 'canna lily',\n",
    "    '90': 'hippeastrum', '91': 'bee balm', '92': 'ball moss', '93': 'foxglove', '94': 'bougainvillea', '95': 'camellia',\n",
    "    '96': 'mallow', '97': 'mexican petunia', '98': 'bromelia', '99': 'blanket flower', '100': 'trumpet creeper', '101': 'blackberry lily'\n",
    "}\n",
    "\n",
    "# Function to preprocess images and run them through the ViT model\n",
    "def preprocess_and_predict(image_bytes):\n",
    "    img = Image.open(io.BytesIO(image_bytes))\n",
    "    inputs = feature_extractor(images=img, return_tensors=\"pt\")\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    # Get predicted class index and map it to the class name\n",
    "    predicted_class_idx = logits.argmax(-1).item()\n",
    "    predicted_class_name = label_to_class[str(predicted_class_idx)]  # Map to class name\n",
    "    return predicted_class_name\n",
    "\n",
    "# Apply preprocessing and prediction to your dataset\n",
    "df_pd['predicted_class'] = df_pd['image'].apply(lambda x: preprocess_and_predict(x['bytes']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [306/306 02:45, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.546500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.628400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.511400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.451500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.257300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.283800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.977300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.949700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.909000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.919200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.874400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.813900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.737400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.641800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.658400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.629200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.594600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.557700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.567700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.479200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.487500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.462500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.479600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.484800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['./final_model/preprocessor_config.json']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import ViTFeatureExtractor\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define the Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, feature_extractor):\n",
    "        self.df = df\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['image']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "\n",
    "        img = None\n",
    "        if isinstance(img_path, str) and os.path.exists(img_path):\n",
    "            img = Image.open(img_path)\n",
    "        elif isinstance(img_path, bytes):\n",
    "            img = Image.open(io.BytesIO(img_path))\n",
    "        elif isinstance(img_path, dict):\n",
    "            if 'bytes' in img_path:\n",
    "                img = Image.open(io.BytesIO(img_path['bytes']))\n",
    "            elif 'path' in img_path:\n",
    "                img = Image.open(img_path['path'])\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected dictionary structure for img_path at index {idx}\")\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported image data type: {type(img_path)}\")\n",
    "\n",
    "        inputs = self.feature_extractor(images=img, return_tensors=\"pt\")\n",
    "        return {'pixel_values': inputs['pixel_values'].squeeze(0), 'label': torch.tensor(label)}\n",
    "\n",
    "# Feature Extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# Split dataset\n",
    "train_df, val_df = train_test_split(df_pd, test_size=0.2, random_state=42)\n",
    "train_dataset = CustomDataset(train_df, feature_extractor)\n",
    "val_dataset = CustomDataset(val_df, feature_extractor)\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,  # ensure model is loaded\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save\n",
    "model.save_pretrained(\"./final_model\")\n",
    "feature_extractor.save_pretrained(\"./final_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayaanhussain/Library/Python/3.11/lib/python/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Evaluating: 100%|██████████| 13/13 [00:06<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 96.08%\n",
      "Validation Precision: [1.  0.  0.5 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 1.  0.5 1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  0.5 1.  1.  1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.5 0.8 1.  1.  0.5]\n",
      "Validation Recall: [1.         0.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.         1.         1.         1.         1.         1.\n",
      " 1.         0.5        0.66666667 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.66666667 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.8        1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.5        1.         1.\n",
      " 0.66666667 1.         1.         0.75       1.         1.\n",
      " 1.         1.         1.        ]\n",
      "Validation F1 Score: [1.         0.         0.66666667 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.66666667 1.         1.         1.         1.\n",
      " 0.         1.         1.         1.         1.         1.\n",
      " 1.         0.5        0.8        1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.8        1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.88888889 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.66666667 1.         1.\n",
      " 0.8        1.         1.         0.85714286 0.66666667 0.88888889\n",
      " 1.         1.         0.66666667]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.50      1.00      0.67         1\n",
      "           6       1.00      1.00      1.00         1\n",
      "           7       1.00      1.00      1.00         2\n",
      "           8       1.00      1.00      1.00         3\n",
      "           9       1.00      1.00      1.00         3\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      1.00      1.00         1\n",
      "          12       1.00      1.00      1.00         4\n",
      "          13       1.00      1.00      1.00         1\n",
      "          14       1.00      1.00      1.00         1\n",
      "          15       1.00      1.00      1.00         2\n",
      "          16       1.00      1.00      1.00         1\n",
      "          17       1.00      1.00      1.00         2\n",
      "          18       1.00      1.00      1.00         3\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          23       0.50      1.00      0.67         1\n",
      "          24       1.00      1.00      1.00         3\n",
      "          25       1.00      1.00      1.00         6\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         3\n",
      "          30       0.00      0.00      0.00         0\n",
      "          31       1.00      1.00      1.00         1\n",
      "          32       1.00      1.00      1.00         3\n",
      "          33       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         1\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       1.00      1.00      1.00         1\n",
      "          38       0.50      0.50      0.50         2\n",
      "          40       1.00      0.67      0.80         3\n",
      "          41       1.00      1.00      1.00         2\n",
      "          42       1.00      1.00      1.00         2\n",
      "          43       1.00      1.00      1.00         3\n",
      "          44       1.00      1.00      1.00         1\n",
      "          45       1.00      1.00      1.00         3\n",
      "          46       1.00      1.00      1.00         1\n",
      "          47       1.00      1.00      1.00         3\n",
      "          48       1.00      1.00      1.00         1\n",
      "          49       1.00      1.00      1.00         3\n",
      "          50       1.00      0.67      0.80         3\n",
      "          51       1.00      1.00      1.00         3\n",
      "          52       1.00      1.00      1.00         3\n",
      "          53       1.00      1.00      1.00         1\n",
      "          54       1.00      1.00      1.00         1\n",
      "          55       1.00      1.00      1.00         2\n",
      "          56       1.00      1.00      1.00         2\n",
      "          57       1.00      1.00      1.00         6\n",
      "          58       1.00      1.00      1.00         1\n",
      "          59       1.00      1.00      1.00         1\n",
      "          60       1.00      1.00      1.00         4\n",
      "          61       1.00      1.00      1.00         4\n",
      "          63       1.00      1.00      1.00         1\n",
      "          64       1.00      1.00      1.00         3\n",
      "          65       1.00      1.00      1.00         1\n",
      "          66       1.00      1.00      1.00         1\n",
      "          67       1.00      1.00      1.00         3\n",
      "          68       1.00      1.00      1.00         5\n",
      "          69       1.00      1.00      1.00         4\n",
      "          70       1.00      1.00      1.00         5\n",
      "          73       1.00      1.00      1.00         3\n",
      "          74       1.00      1.00      1.00         4\n",
      "          75       1.00      1.00      1.00         1\n",
      "          77       1.00      1.00      1.00         1\n",
      "          78       1.00      0.80      0.89         5\n",
      "          79       1.00      1.00      1.00         3\n",
      "          80       1.00      1.00      1.00         1\n",
      "          81       1.00      1.00      1.00         1\n",
      "          82       1.00      1.00      1.00         4\n",
      "          84       1.00      1.00      1.00         2\n",
      "          85       1.00      1.00      1.00         2\n",
      "          86       1.00      1.00      1.00         2\n",
      "          87       1.00      1.00      1.00         2\n",
      "          89       1.00      0.50      0.67         4\n",
      "          90       1.00      1.00      1.00         3\n",
      "          91       1.00      1.00      1.00         4\n",
      "          92       1.00      0.67      0.80         3\n",
      "          93       1.00      1.00      1.00         2\n",
      "          94       1.00      1.00      1.00         2\n",
      "          95       1.00      0.75      0.86         4\n",
      "          96       0.50      1.00      0.67         1\n",
      "          97       0.80      1.00      0.89         4\n",
      "          99       1.00      1.00      1.00         2\n",
      "         100       1.00      1.00      1.00         2\n",
      "         101       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.96       204\n",
      "   macro avg       0.95      0.95      0.94       204\n",
      "weighted avg       0.98      0.96      0.97       204\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the saved model and feature extractor\n",
    "model = ViTForImageClassification.from_pretrained(\"./final_model\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# Assuming CustomDataset is defined as earlier\n",
    "train_dataset = CustomDataset(train_df, feature_extractor)\n",
    "val_dataset = CustomDataset(val_df, feature_extractor)\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Move model to device (if you're using GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(dataloader, model):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # No gradients needed for evaluation\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            # Move inputs to the device (GPU/CPU)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(pixel_values)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Get predictions (argmax to get class index)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Append predictions and labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average=None)  # Precision for each class\n",
    "    recall = recall_score(all_labels, all_preds, average=None)  # Recall for each class\n",
    "    f1 = f1_score(all_labels, all_preds, average=None)  # F1 score for each class\n",
    "\n",
    "    # You can also use classification_report for a more detailed output\n",
    "    report = classification_report(all_labels, all_preds)\n",
    "\n",
    "    return accuracy, precision, recall, f1, report\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_accuracy, val_precision, val_recall, val_f1, report = evaluate_model(val_dataloader, model)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "print(f\"Validation F1 Score: {val_f1}\")\n",
    "\n",
    "# Print the detailed classification report (including precision, recall, and F1 score for each class)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 85877094\n",
      "Trainable parameters: 85877094\n",
      "Non-trainable parameters: 0\n",
      "Number of dense (fully connected) layers: 73\n"
     ]
    }
   ],
   "source": [
    "# Check the number of parameters of the model\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {num_params}\")\n",
    "\n",
    "# Trainable parameters (those that require gradients)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "\n",
    "# Non-trainable parameters (those that do not require gradients)\n",
    "non_trainable_params = num_params - trainable_params\n",
    "print(f\"Non-trainable parameters: {non_trainable_params}\")\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Function to count dense layers in a model\n",
    "def count_dense_layers(model):\n",
    "    dense_layers = 0\n",
    "    # Iterate over all layers in the model\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            dense_layers += 1\n",
    "    return dense_layers\n",
    "\n",
    "# Count and print the number of dense layers\n",
    "dense_layer_count = count_dense_layers(model)\n",
    "print(f\"Number of dense (fully connected) layers: {dense_layer_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: LABEL_42\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "\n",
    "# Load the model and processor (updated method, since FeatureExtractor is deprecated)\n",
    "model = ViTForImageClassification.from_pretrained(\"./final_model\")\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# Load and preprocess a random image\n",
    "image_path = \"swordLily.jpg\"  # Replace this with your image path\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_idx = logits.argmax(-1).item()\n",
    "\n",
    "predicted_label = model.config.id2label[predicted_class_idx]\n",
    "print(f\"Predicted label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERROR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:13<00:00, 14.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Confused Class Pairs:\n",
      "   True Label  Predicted Label  Count\n",
      "0          31                1      1\n",
      "1          32                2      1\n",
      "2          42               83      1\n",
      "3          66               86      1\n",
      "4          75               19      1\n",
      "5          75               24      1\n",
      "6          78               31      1\n",
      "7          81               82      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for item in tqdm(val_dataset):\n",
    "    pixel_values = item['pixel_values'].unsqueeze(0).to(device)  \n",
    "    label = item['label']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "    all_preds.append(pred)\n",
    "    all_labels.append(label)\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "most_confused = []\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm)):\n",
    "        if i != j and cm[i][j] > 0:\n",
    "            most_confused.append((i, j, cm[i][j]))\n",
    "\n",
    "confusion_df = pd.DataFrame(most_confused, columns=[\"True Label\", \"Predicted Label\", \"Count\"])\n",
    "confusion_df = confusion_df.sort_values(by=\"Count\", ascending=False).head(10)\n",
    "\n",
    "print(\"\\nTop 10 Most Confused Class Pairs:\")\n",
    "print(confusion_df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
